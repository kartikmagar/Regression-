{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Theory Questions"
      ],
      "metadata": {
        "id": "1cualqEzuwGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What is Simple Linear Regression?\n",
        "\n",
        "->  Simple Linear Regression is a statistical method used to model the relationship between two variables: one independent variable (X) and one dependent variable (Y). It fits a straight line (Y = mX + c) through the data points to predict the value of Y based on X.\n",
        "\n",
        "\n",
        "### 2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "-> Linearity: Relationship between X and Y is linear.\n",
        "Independence: Observations are independent of each other.\n",
        "Homoscedasticity: Constant variance of errors.\n",
        "Normality: Residuals (errors) are normally distributed.\n",
        "\n",
        "\n",
        "### 3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "\n",
        "-> m is the slope of the regression line. It represents the change in Y for a one-unit change in X.\n",
        "\n",
        "\n",
        "### 4. What does the intercept c represent in the equation Y = mX + c?\n",
        "\n",
        "-> c is the intercept of the line. It indicates the value of Y when X = 0.\n",
        "\n",
        "\n",
        "### 5. How do we calculate the slope m in Simple Linear Regression?\n",
        "Using the least squares method:\n",
        "\n",
        "-> m = (n * Σ(xy) - Σx * Σy) / (n * Σ(x²) - (Σx)²)\n",
        "\n",
        "\n",
        "### 6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "-> To find the best-fitting line by minimizing the sum of squared differences (errors) between the actual Y values and predicted Y values.\n",
        "\n",
        "\n",
        "### 7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        "-> R² measures the proportion of the variance in the dependent variable that is predictable from the independent variable.\n",
        "R² = 1: perfect fit\n",
        "R² = 0: no relationship\n",
        "\n",
        "\n",
        "### 8. What is Multiple Linear Regression?\n",
        "\n",
        "-> Multiple Linear Regression involves more than one independent variable to predict the dependent variable. The equation is: \\(Y=b_{0}+b_{1}X_{1}+b_{2}X_{2}+\\dots +b_{n}X_{n}\\)\n",
        "\n",
        "\n",
        "### 9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "-> Simple Linear Regression uses one independent variable.\n",
        "Multiple Linear Regression uses two or more independent variables.\n",
        "\n",
        "\n",
        "### 10. What are the key assumptions of Multiple Linear Regression?\n",
        "Same as Simple Linear Regression plus:\n",
        "\n",
        "-> No multicollinearity between predictors.\n",
        "No autocorrelation.\n",
        "The model is correctly specified (no missing variables).\n",
        "\n",
        "\n",
        "### 11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "-> Heteroscedasticity means non-constant variance of errors. It can lead to inefficient estimates and incorrect conclusions about predictor significance.\n",
        "\n",
        "\n",
        "### 12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "-> Remove highly correlated variables.\n",
        "Use Principal Component Analysis (PCA).\n",
        "Apply Ridge or Lasso regression.\n",
        "Combine correlated predictors.\n",
        "\n",
        "\n",
        "### 13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "-> One-hot encoding\n",
        "Label encoding\n",
        "Binary encoding\n",
        "Ordinal encoding\n",
        "\n",
        "\n",
        "### 14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "-> Interaction terms allow modeling interactions between variables, helping to capture combined effects (e.g., X1 * X2) that wouldn't be explained by individual variables alone.\n",
        "\n",
        "\n",
        "### 15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "-> In Simple Linear Regression, intercept is the expected value of Y when X = 0.\n",
        "In Multiple Linear Regression, it's the expected value of Y when all Xs = 0 (which may not be meaningful if 0 is outside the data range).\n",
        "\n",
        "\n",
        "### 16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "-> The slope shows how much Y changes per unit change in X. A significant slope (low p-value) means X has a statistically significant impact on Y.\n",
        "\n",
        "\n",
        "### 17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "-> The intercept represents the predicted value of Y when all X variables are 0. It gives a baseline and helps interpret the overall model, especially when zero is meaningful for predictors.\n",
        "\n",
        "\n",
        "### 18. What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "-> R² doesn't indicate causation.\n",
        "It can increase with added variables, even if they're irrelevant.\n",
        "It doesn't detect overfitting or model bias.\n",
        "Doesn't measure prediction accuracy on new data.\n",
        "\n",
        "\n",
        "### 19. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "-> A large standard error indicates uncertainty in the estimate of the coefficient. It suggests the coefficient might not be significantly different from zero.\n",
        "\n",
        "\n",
        "### 20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "-> Identification: Residuals fan out or form a pattern (non-random scatter).\n",
        "Importance: It violates regression assumptions, affecting standard errors, leading to unreliable hypothesis tests and confidence intervals.\n",
        "\n",
        "\n",
        "### 21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "-> It means the model has many variables, but some may be irrelevant. Adjusted R² penalizes for adding unnecessary variables, so a low adjusted R² suggests overfitting.\n",
        "\n",
        "\n",
        "### 22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "-> Ensures equal weighting for variables.\n",
        "Helps in convergence for optimization algorithms.\n",
        "Necessary for regularization methods like Ridge/Lasso.\n",
        "\n",
        "\n",
        "### 23. What is polynomial regression?\n",
        "\n",
        "-> Polynomial regression models the relationship between X and Y as an nth-degree polynomial:\n",
        "(Y=\\beta _{0}+\\beta _{1}X+\\beta _{2}X^{2}+\\dots +\\beta _{n}X^{n}+\\epsilon \\)\n",
        "\n",
        "\n",
        "### 24. How does polynomial regression differ from linear regression?\n",
        "\n",
        "-> Linear regression models a straight-line relationship. Polynomial regression models a curved relationship using higher-degree terms.\n",
        "\n",
        "\n",
        "### 25. When is polynomial regression used?\n",
        "\n",
        "-> When data shows a non-linear pattern.\n",
        "When the relationship cannot be well-fit by a straight line.\n",
        "\n",
        "\n",
        "### 26. What is the general equation for polynomial regression?\n",
        "\n",
        "-> y = b₀ + b₁x + b₂x² + ... + bₙxⁿ + ε\n",
        "n is the degree of the polynomial.\n",
        "\n",
        "\n",
        "### 27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "-> Yes. It becomes multivariate polynomial regression, involving interaction and polynomial terms of multiple independent variables.\n",
        "\n",
        "\n",
        "### 28. What are the limitations of polynomial regression?\n",
        "\n",
        "-> High degree polynomials can overfit.\n",
        "Poor extrapolation beyond the training data.\n",
        "Computationally expensive for high degrees.\n",
        "Interpretability decreases as degree increases.\n",
        "\n",
        "\n",
        "### 29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "-> Cross-validation\n",
        "Adjusted R²\n",
        "AIC/BIC\n",
        "Residual analysis\n",
        "Validation set performance (RMSE, MAE)\n",
        "\n",
        "\n",
        "### 30. Why is visualization important in polynomial regression?\n",
        "It helps:\n",
        "\n",
        "-> Understand fit quality.\n",
        "Detect overfitting/underfitting.\n",
        "Communicate model behavior clearly.\n",
        "\n"
      ],
      "metadata": {
        "id": "jmRVj2Uivp5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 31. How is polynomial regression implemented in Python?\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())"
      ],
      "metadata": {
        "id": "6hOhoBu05Gcr"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}